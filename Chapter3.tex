\section{Transforms}


\subsection{a}
\subsection{b}
\subsection{The Moment Generating Function}
\exercise
\exercise
\exercise
\exercise
\exercise
\begin{enumerate}[label=(\alph*)]
    \item Show that if \( X \sim N(\mu, \sigma^2) \), then \( \mathbb{E}[X] = \mu \) and \( \text{Var}(X) = \sigma^2 \).
    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. Show that \( X_1 + X_2 \) is normally distributed, and find the mean and variance of \( X_1 + X_2 \).
    \item Let \( X \sim N(0,\sigma^2) \). Show that for \( n = 0, 1, 2, \ldots \),
    \[
    \mathbb{E}[X^{2n+1}] = 0, 
    \]
    and 
    \[
    \mathbb{E}[X^{2n}] = (2n-1)!! \cdot \sigma^{2n} = 1 \cdot 3 \cdot 5 ... \cdot (2n-1) \cdot \sigma^{2n}.
    \]
    Here, \( (2n-1)!! \) denotes the double factorial of \( 2n-1 \).
\end{enumerate}
\solution
\begin{enumerate}[label=(\alph*)]
    \item Given a normal random variable \( X \sim N(\mu, \sigma^2) \), its characteristic function \( \psi_X(t) \) is expressed as:
    \[
    \psi_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
    \]
    The expected value \( \mathbb{E}[X] \) is the coefficient of \( t \) in the Taylor expansion of \( \psi_X(t) \) around \( t=0 \), which yields:
    \[
    \mathbb{E}[X] = \left. \frac{d}{dt}\psi_X(t) \right|_{t=0} = \mu.
    \]
    To find the variance \( \text{Var}(X) \), we compute the second derivative of \( \psi_X(t) \) at \( t=0 \):
    \[
    \text{Var}(X) = \left. \frac{d^2}{dt^2}\psi_X(t) \right|_{t=0} - (\mu)^2 = \sigma^2.
    \]

    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. To show that the sum \( X_1 + X_2 \) is also normally distributed and to find its parameters, consider their moment generating functions:

    \[
    \Psi_{X_1}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2}, \quad \Psi_{X_2}(t) = e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Since \( X_1 \) and \( X_2 \) are independent, the MGF of their sum is the product of their MGFs:
    
    \[
    \Psi_{X_1 + X_2}(t) = \Psi_{X_1}(t) \cdot \Psi_{X_2}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2} \cdot e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Simplify by combining the exponents:
    
    \[
    \Psi_{X_1 + X_2}(t) = e^{(\mu_1 + \mu_2) t + \frac{1}{2} (\sigma_1^2 + \sigma_2^2) t^2}.
    \]
    
    This is the MGF of a normal distribution with mean \( \mu_1 + \mu_2 \) and variance \( \sigma_1^2 + \sigma_2^2 \). Therefore, \( X_1 + X_2 \) follows a normal distribution \( N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \).
    \item 

    Let \( X \sim N(0, \sigma^2) \). The characteristic function \( \psi_X(t) \), which also serves as the moment generating function in this context, is given by:
    \[
    \psi_X(t) = e^{\frac{1}{2} \sigma^2 t^2}.
    \]
    Expanding \( \psi_X(t) \) using a Taylor series around \( t = 0 \) results in:
    \[
    \psi_X(t) = \sum_{n=0}^{\infty} \frac{\frac{1}{2}\sigma^2 t^2}{n!} t^{2n} = 1 + \frac{1}{2}\sigma^2 t^2 + \frac{(\frac{1}{2}\sigma^2 t^2)^2}{2!} + \frac{(\frac{1}{2}\sigma^2 t^2)^3}{3!} + \ldots
= 1 + \frac{\sigma^2 t^2}{2 } + \frac{\sigma^4 t^4}{2^2 \cdot 2!} + \frac{\sigma^6 t^6}{2^3 \cdot 3!} + \ldots
    \]
    
    This series only contains even powers of \( t \), confirming that all coefficients of odd powers of \( t \) are zero, thus:
    \[
    \mathbb{E}[X^{2n+1}] = 0
    \]
    for all odd powers \( 2n+1 \). This occurs because the derivatives of \( \psi_X(t) \) at \( t=0 \) for odd orders are zero, as each term in the expansion of \( \psi_X(t) \) contains even powers.
    
    For even powers, consider the coefficient of \( t^{2n} \) in the Taylor expansion:
    \[
    \mathbb{E}[X^{2n}] = \left. \frac{d^{2n}}{dt^{2n}} \psi_X(t) \right|_{t=0} = \left. \frac{d^{2n}}{dt^{2n}} \left( \sum_{k=0}^{\infty} \frac{1}{k!} \left(\frac{1}{2}\sigma^2 t^2\right)^k \right) \right|_{t=0}
    \]
    
    To see why \( \mathbb{E}[X^{2n}] \) equals \((2n-1)!! \sigma^{2n}\), take the \(2n\)-th derivative:
    \[
    \mathbb{E}[X^{2n}] = \frac{1}{n!} \left(\frac{1}{2} \sigma^2\right)^n \cdot 2^n \cdot (2n)! = \sigma^{2n} \cdot (2n-1)!!
    \]
    This computation correctly reflects the product of the double factorial \((2n-1)!!\) which is the product of all odd numbers up to \((2n-1)\), resulting in:
    \[
    (2n-1)!! = 1 \cdot 3 \cdot 5 \cdot \ldots \cdot (2n-1) \cdot (\sigma^{2n}).
    \]
    
\end{enumerate}


\exercise

\begin{itemize}
    \item[(a)] Show that if \( X \sim N(0, 1) \) then \( X^2 \sim \chi^2(1) \) by computing the moment generating function (MGF) of \( X^2 \), that is, by showing that
    \[
    \psi_{X^2}(t) = \mathbb{E}[\exp(tX^2)] = \frac{1}{\sqrt{1-2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]

    \item[(b)] Show that if \( X_1 \sim N(0, 1) \) and \( X_2 \sim N(0, 1) \) are independent, then \( X_1^2 + X_2^2 \) is distributed as \( \chi^2(2) \) (which is equivalent to an exponential distribution with mean 2).
\end{itemize}

\solution
\begin{enumerate}[label=(\alph*)]
    \item
    
    Begin by recognizing the integral for the MGF:
    \[
    \psi_{X^2}(t) = \int_{-\infty}^{\infty} e^{tx^2} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{x^2(t - \frac{1}{2})} \, dx.
    \]
    This integral converges for \(t < \frac{1}{2}\). Transform \( x \) to eliminate the variable change explicitly:
    \[
    \frac{d(x\sqrt{1 - 2t})}{dx} = \sqrt{1 - 2t}, \quad dx = \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}}
    \]
    Substitute directly:
    \[
    \psi_{X^2}(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x\sqrt{1 - 2t})^2}{2}} \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}} = \frac{1}{\sqrt{1 - 2t}}.
    \]
    The integral of the standard normal density over the transformed variable is 1, leading to the final MGF expression for \( X^2 \).
    \item[(b)] Given that \(X_1 \sim N(0, 1)\) and \(X_2 \sim N(0, 1)\) are independent, to show that \(X_1^2 + X_2^2\) is distributed as \(\chi^2(2)\), consider the moment generating functions (MGFs) of \(X_1^2\) and \(X_2^2\), which are:
    \[
    \psi_{X_1^2}(t) = \psi_{X_2^2}(t) = \frac{1}{\sqrt{1 - 2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]
    Since \(X_1^2\) and \(X_2^2\) are independent, the MGF of their sum, \(X_1^2 + X_2^2\), is the product of their MGFs:
    \[
    \psi_{X_1^2 + X_2^2}(t) = \psi_{X_1^2}(t) \cdot \psi_{X_2^2}(t) = \left(\frac{1}{\sqrt{1 - 2t}}\right)^2 = \frac{1}{1 - 2t}.
    \]
    This MGF, \(\frac{1}{1 - 2t}\), is the MGF of a \(\chi^2\) distribution with 2 degrees of freedom. The \(\chi^2(2)\) distribution is also known to be equivalent to an exponential distribution with mean 2, confirming the distribution of \(X_1^2 + X_2^2\).
\
\end{enumerate}


\subsection{The Characteristic Function}

\exercise

\begin{enumerate}[label=(\alph*)]
    \item
    For a Bernoulli random variable \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = q + p e^{it}, \quad \text{where } q = 1-p.
    \]
    
    \item 
    
    For a Binomial random variable \(Y \sim \text{Bin}(n,p)\):
    \[
    \varphi_{\text{Bin}(n,p)}(t) = (q + p e^{it})^n.
    \]
    
    \item 
    
    For a compound Poisson random variable \(Z\) with rate \(\lambda\) and jump size distribution \(C\):
    \[
    \varphi_{C}(t) = \frac{p}{1 - q e^{ist}},
    \]
    assuming a specific relationship between the parameters \(p\) and \(q\), and \(s\).
    
    \item 
    
    For a compound Poisson random variable \(W\) with intensity \(m\) and jump size distribution \(P\):
    \[
    \varphi_{P \ast \theta(m)}(t) = \exp\left[m(e^{it} - 1)\right].
    \]
\end{enumerate}
\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
    \item 
    \textbf{Bernoulli Distribution} \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = \mathbb{E}[e^{itX}] = \sum_{x=0}^1 e^{itx} \Pr(X = x) = e^{it \cdot 0} \Pr(X=0) + e^{it \cdot 1} \Pr(X=1) = (1-p) + pe^{it}.
    \]
    This is exactly the expression given: \(q + p e^{it}\), where \(q = 1-p\).
    
    \item 
    \textbf{Binomial Distribution} \(Y \sim \text{Bin}(n,p)\):
    The characteristic function of a sum of independent identically distributed random variables (by the property often called the \emph{factorization property}) is:
    \[
    \varphi_{\text{Bin}(n,p)}(t) = [\varphi_{\text{Be}(p)}(t)]^n = (q + pe^{it})^n.
    \]
    This uses the property that the characteristic function of the sum of independent random variables is the product of their characteristic functions.
    
    \item 
    \textbf{Geometric Distribution}:
    \[
    \varphi_{X}(t) = \mathbb{E}[e^{itX}] = \sum_{x=0}^\infty e^{itx} \Pr(X = x) = \sum_{x=0}^\infty e^{itx} \frac{p q^x}{1-q} = \frac{p}{1-qe^{it}},
    \]
    where we used the formula for the sum of a geometric series \(\sum_{x=0}^\infty ar^x = \frac{a}{1-r}\) applied to \(e^{it}\) as \(r\).
    

\item 
\textbf{Compound Poisson Distribution} (\( W \)) with intensity \( m \) and jump size distribution \( P \):

The compound Poisson variable \( W \) can be expressed as \( W = \sum_{k=1}^N X_k \), where \( N \sim \text{Poisson}(m) \) and \( X_k \) are iid random variables from the distribution \( P \). The characteristic function \( \varphi_{W}(t) \) is given by the expectation:
\[
\varphi_{W}(t) = \mathbb{E}[e^{itW}].
\]

Given \( W \) conditioned on \( N \) being equal to \( n \), the sum \( W = X_1 + X_2 + \cdots + X_n \) and the \( X_k \)'s are independent. So, we write:
\[
\mathbb{E}[e^{itW} \mid N=n] = \mathbb{E}[e^{it(X_1 + X_2 + \cdots + X_n)}] = \prod_{k=1}^n \mathbb{E}[e^{itX_k}] = (\varphi_P(t))^n,
\]
where \( \varphi_P(t) \) is the characteristic function of the distribution \( P \).

The unconditional expectation is:
\[
\varphi_{W}(t) = \sum_{n=0}^\infty \mathbb{E}[e^{itW} \mid N=n] \Pr(N = n) = \sum_{n=0}^\infty (\varphi_P(t))^n \frac{e^{-m} m^n}{n!}.
\]
Using the Taylor series expansion for the exponential function, we have:
\[
\varphi_{W}(t) = e^{-m} \sum_{n=0}^\infty \frac{[m \varphi_P(t)]^n}{n!} = e^{-m} e^{m \varphi_P(t)} = \exp[m(\varphi_P(t) - 1)].
\]

This directly ties into the idea you suggested, where each \( e^{itx} \) term is weighted by its Poisson probability, which then sums to form the exponential series representation of \( \varphi_{W}(t) \).




\exercise
\exercise
\begin{enumerate}[label=(\alph*)]
    \item Calculate the mean and variance of the Binomial distribution using its characteristic function.
    \item Calculate the mean and variance of the Poisson distribution using its characteristic function.
    \item Calculate the mean and variance of the Uniform distribution using its characteristic function.
    \item Calculate the mean and variance of the Exponential distribution using its characteristic function.
\end{enumerate}

\solution
\begin{enumerate}[label=(\alph*)]
    \item
    \textbf{Binomial Distribution}:
    \[
    \text{Characteristic Function:} \quad \varphi_X(t) = (1-p + pe^{it})^n
    \]
    \text{Expansion of} \( e^{it} \):
    \[
    e^{it} \approx 1 + it - \frac{t^2}{2}
    \]
    \text{Substitute and apply multinomial theorem:}
    \[
    \varphi_X(t) = (1-p + p(1 + it - \frac{pt^2}{2}))^n
    \]
    \text{Expand using multinomial coefficients:}
    \[
    \varphi_X(t) \approx \sum_{x,y,z}^n \binom{n}{x,y,z} (1-p)^x \left(pit\right)^y \left(-\frac{pt^2}{2}\right)^{z}
    \]
    \text{Relevant terms up to} \( t^2 \):
    \[
    \varphi_X(t) \approx \binom{n}{n,0,0}(1-p)^n + \binom{n}{n-1,1,0}(1-p)^{n-1}(pit) + \binom{n}{n-2,0,2}(1-p)^{n-2}\left(-\frac{pt^2}{2}\right)
    \]
    \text{Mean} \( E[X] \):
    \[
    E[X] = np
    \]
    \text{Variance} \( \operatorname{Var}(X) \):
    \[
    \operatorname{Var}(X) = np(1-p)
    \]

    \item
\textbf{Poisson Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = e^{\lambda (e^{it} - 1)}
\]
\text{Expansion of} \( e^{it} \):
\[
e^{it} \approx 1 + it - \frac{t^2}{2}
\]
\text{Substitute and expand:}
\[
\varphi_X(t) = e^{\lambda \left((1 + it - \frac{t^2}{2}) - 1\right)} = e^{\lambda (it - \frac{t^2}{2})}
\]
\text{Applying Taylor expansion to} \( e^{\lambda (it - \frac{t^2}{2})} \):
\[
\varphi_X(t) \approx 1 + \lambda (it - \frac{t^2}{2}) + \frac{\lambda^2}{2} (it - \frac{t^2}{2})^2 + \ldots
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + (\lambda^2 + \lambda) it - \frac{\lambda t^2}{2}
\]
\text{Mean} \( E[X] \):
\[
E[X] = \lambda
\]
\text{Second moment} \( E[X^2] \):
\[
E[X^2] = \lambda^2 - \lambda
\]
\text{Variance} \( \operatorname{Var}(X) \):
\[
\operatorname{Var}(X) = \lambda
\]


\item
\textbf{Uniform Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = \frac{e^{itb} - e^{ita}}{it(b-a)}
\]
\text{Expansions of} \( e^{itb} \) \text{and} \( e^{ita} \) (   \text{remember, we will divide by} \(it(b-a) \) \text{so we need terms up to} \(t^3\) ) : 
\[
e^{itb} \approx 1 + itb - \frac{t^2b^2}{2} - i\frac{t^3b^3}{6}, \quad e^{ita} \approx 1 + ita - \frac{t^2a^2}{2} - i\frac{t^3a^3}{6}
\]
\text{Substitute and simplify:}
\[
\varphi_X(t) = \frac{(1 + itb - \frac{t^2b^2}{2}- i\frac{t^3b^3}{6}) - (1 + ita - \frac{t^2a^2}{2}) - i\frac{t^3a^3}{6}}{it(b-a)}
\]
\[
\varphi_X(t) \approx \frac{1}{it(b-a)} \Bigl[ (1-1)+ it(b-a) -\dfrac{t^2}{2}(b^2-a^2) -i\dfrac{t^3}{6}(b^3-a^3) \Bigr] = \Bigl[ 0 + 1 +it\dfrac{b+a}{2}  + \dfrac{t^2}{6} (b^2+a^2 -ab) \Bigr]
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + it\frac{b+a}{2} - \frac{t^2(b^2 + a^2-ab)^2}{6}
\]
\text{Mean} \( E[X] \):
\[
E[X] = \frac{b+a}{2}
\]
\text{Second moment} \( E[X^2] \):
\[
E[X] = \frac{b^2+a^2-ab}{3}
\]
\text{Variance} \( \operatorname{Var}(X) \):
\[
\operatorname{Var}(X) = E[X^2] - E[X]^2 =  \frac{4(b^2+a^2-ab)}{4\cdot3} - \frac{3\cdot(b+a)^2}{3\cdot2^2}  = \frac{(b-a)^2}{12}
\]


\item
\textbf{Exponential Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = \frac{1}{1 - it/\lambda}
\]
\text{Expand using Gemoretric series}
\[
\frac{1}{1-x} = 1 +x + x^2 +x^3... \therefore \varphi_X(t) \approx 1 + it/\lambda + (it/\lambda)^2 + o(t^2)
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + it \dfrac{1}{\lambda} - \frac{t^2}{2} \frac{2}{\lambda^2}
\]
\[
E[X] = \frac{1}{\lambda} \hspace{15pt} E[X^2] = \frac{2}{\lambda^2} \hspace*{15pt} \operatorname{Var}(X) = \frac{1}{\lambda^2}
\]


\item
\textbf{Standard Normal Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = e^{-\frac{t^2}{2}}
\]
\text{Apply Taylor expansion to} \( e^{-\frac{t^2}{2}} \):
\[
\varphi_X(t) \approx 1 - \frac{t^2}{2} + \frac{t^4}{8} - \frac{t^6}{48} + \ldots
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 - \frac{t^2}{2}
\]
\text{Which yields}:
\[
E[X] = 0 \hspace*{15pt} \operatorname{Var}(X) = 1
\]




\end{enumerate}

\exercise
\exercise
\exercise
Use Theorem 4.9 to show that \( \varphi_{C(m,a)}(t) = e^{itm} \varphi_X(at) = e^{itm - a|t|} \)
\solution
Theorem 4.9 states that \[
\phi_{aX+b} (t) = e^{itb} \cdot \phi_X(at)
\]
Physics teaches us that a cauchy distribution is the dist of a $x$-intersect of a random ray going through the point $C(m, a)$

Changing m is the same as moving the intersect by m, and changing a is the same as multiplying the intersect point, taking acount the scaling already done by m.

It is therefore obvious that \[
\phi_{C(m, a)} (t) = e^{itm} \cdot \phi_X(at) =e^{itm} \cdot e^{ -\|at \| }  = \exp(itm - \|at\| )
\] 



% https://researchers.ms.unimelb.edu.au/~xgge@unimelb/Files/Teaching/Advanced%20Probability%20(Semester%201%202020)/Deriving%20the%20Inversion%20Formula.pdf
% cool proof inversion formula

\exercise
Show that if $X, Y$ are iid, then $X-Y$ has a symetric distribution:

\solution
yet again prove something obvious but with characteristic functions. If $X \stackrel{d}{=} Y$ then 


\[
\phi_{X-Y} (t) = \text{(independent)} =  \phi_X (t) \cdot  \phi_Y(-t) = \text{(equidistributed)} = \phi_X(t) \cdot \phi_X(-t) =   
\phi_X(t) \cdot \overline{\phi_X(-t)} = \text{real}  
\]


\exercise
Show that one cannot find i.i.d R.V $X$ and $Y$ such that $X-Y \in U(-1, 1) $ 
\solution

We know that 


\[
\phi_{X-Y} (t) = \text{(independent)} =  \phi_X (t) \cdot  \phi_Y(-t) = \text{(equidistributed)} = \phi_X(t) \cdot \phi_X(-t) =   
\phi_X(t) \cdot \overline{\phi_X(-t)} = \| \phi_X (t) \|^2   
\]

Which is strictly positive, however this does not hold true for 
\[
\phi_{U(-1, 1)} = \frac{\sin(t)}{t}    
\]


\subsection{Distributions with random parameters}

\exercise
\begin{enumerate}
    \item
    if $M = m$, then $X$ is $Po(m)$-distributed. However, $M$ is $Exp(a)$ distributed. ie
    \[     X | M = m \sim  Po(m) \text{ with } M \sim Exp(a) 
    \]
    Calculate the distribution of $X$
    \item    \[     X | M = m \sim  Po(m) \text{ with } M \sim \Gamma(p,a) 
    \]
    Calculate the distribution of $X$
\end{enumerate}

\solution
\begin{enumerate}
    \item 
    \[
        g_X (t) = E \left[ t^X \right] = E \left[ E \left[ t^X |  M \right] \right] = E \left[ g_{Po(M)}(t)\right] = E\left[e^{M(t-1)}\right]
    \]

    This is a moment generating function, more preciely

    \[
        E\left[e^{M(t-1)}\right] \sim \psi_{M}(t-1) = \psi_{Exp(a)}(t-1) = \frac{1}{1-a(e^t-1)} = \frac{\dfrac{1}{1+a}}{1-\dfrac{a}{a+1}(e^t-1)} \sim Ge(\dfrac{1}{1+a})
    \]

 \item
    
    \[
        g_X (t) = E \left[ t^X \right] = E \left[ E \left[ t^X |  M \right] \right] = E \left[ g_{Po(M)}(t)\right] = E\left[e^{M(t-1)}\right]
    \]

    This is also moment generating function, more preciely

    \[
        E\left[e^{M(t-1)}\right] \sim \psi_{M}(t-1) = \psi_{\Gamma(p,a)}(t-1) = \frac{1}{(1-at)^p} =   \]

    \[
X \sim \text{NegBin}\left(p, \frac{1}{a+1}\right)
\]
\end{enumerate}


\exercise 

\begin{enumerate}
    \item $X$ is $N(0,1/ \Sigma^2)$ distributed, where $\Sigma^2$ is $\Gamma(\frac{n}{2} , \frac{2}{n}) $ distributed
\end{enumerate}


\solution
Really don't know how to do it since they dont provide the MGF for Student t



\subsection{Sums of a Random Number of Random Numbers}

\exercise
Compute $E[S_N^2] $ and prove $Var(S_N) = E[N] \cdot Var(X) + E[X]^2 \cdot Var(N)$ .

\solution

\[
    E S_N^2 = \sum E(S_N^2 | N = n) \cdot P(N=n) = \sum E(S_n^2) \cdot P(N=n) = \sum E[(X_1 + ... X_n)^2] \cdot P(N=n) =
\]
\[
    \sum \biggl(E[X^2] \cdot  n + E[X]^2 \cdot n(n-1) \biggr) P(N=n) = E[X^2]\sum n \cdot P(N=n) + E[X]^2 \sum (n^2-n)P(N=n)
\]
\[
  E[X^2]E[N]  + E[X]^2(E[N^2] - E[N])
\]
We know that \(Var(S_N) = E[S_N^2]-E[S_N]^2\), and using the result from (a) we get
\[
    Var(S_N) = E[X^2]E[N]  +E[X]^2(E[N^2] - E[N]) - E[X]^2 E[N]^2
\]
Rearanging gives us:
\[
    Var(S_N) = E[N](E[X^2] - E[X]^2)  +E[X]^2(E[N^2] - E[N]^2) = E[N]Var(X)+E[X^2]Var(N) 
\]

\exercise
Charlie bets on 13 on a $(0,1...36)$ roulette table untill they win ($N$ times), and then bets $N$ times again on 36 in the second round. Find the generating function of their loss in the second round
Also find it for the the overall loss

\solution
Let \(X = Y_1 + ... Y_n\) where $N \sim F(\frac{1}{37})$. First lets calculate $g_Y(t)$:


\[
    g_Y(t) = E[t^Y] = \sum t^y P(Y=y) = t^1 \frac{36}{37} + t^{-35} \frac{1}{37}
\]
Knowing that $N$ the number of plays until a win is First time-distributed, we get
\[
    g_X(t) = g_N(g_Y(t)) = g_N(t \frac{36}{37} + t^{-35} \frac{1}{37}) = \frac{p(t \frac{36}{37} + t^{-35} \frac{1}{37})}{1-q(t \frac{36}{37} + t^{-35} \frac{1}{37})} = \frac{\frac{1}{37}(36t + t^{-35} )}{37-\frac{36}{37}(36t + t^{-35} )}
     \]

As a sanity check, the we can take its derivative and make sure the expected loss is 1.
\[ 
    \frac{\frac{1}{37}(36 + -35 \cdot 1 )}{37-\frac{36}{37}(36 + -35 \cdot 1)} = 1
\]

For the first round, Charlie will lose 1 dollar untill they win, and get 35 dollars, ie $L = Y_1 + Y_2 +... Y_N -36$ where $Y_k = 1$. ie the loss $L$ if they play $n$ times is $L = n-36$ dollars. $N$ is still $F(\frac{1}{37})$-distributed
\[
    g_L(t) = g_{N-36}(g_Y(t)) = g_{N-36}(t) = t^{-36}g_N(t) = t^{-36} \frac{\frac{1}{37}}{1 -\frac{36}{37}t}
\]
Evaluating its derivative when $t=1$ yields the expected loss is 1 here too. Because of linearity of expectation, despite these being obviously dependent, the final loss is still just these added up.
\vspace*{100pt}

\exercise
Using the property of \(\psi_{S_N}(t) = g_N(\psi_{X}(t))\), prove;

\begin{enumerate}
    \item \(E[S_N] = E[N]E[X]\)
    \item \(Var(S_N) = E[N]Var[X] + E[X]^2Var[N]\)
\end{enumerate}

\solution

\begin{enumerate}
    \item The expectation $E[S_N]$ is obtained by taking the first derivative of $\psi_{S_N}(t)$ with respect to $t$ and then evaluating at $t=0$:
    \[
    E[S_N] = \left.\frac{d}{dt}\psi_{S_N}(t)\right|_{t=0} = \left.\frac{d}{dt} g_N(\psi_X(t))\right|_{t=0}.
    \]
    Applying the chain rule, we get:
    \[
    E[S_N] = g_N'(\psi_X(0)) \cdot \psi_X'(0).
    \]
    Since $\psi_X(0) = 1$ and knowing that $\psi_X'(0) = E[X]$ (from the properties of MGFs),
    \[
    E[S_N] = g_N'(1) \cdot E[X].
    \]
    The first derivative of $g_N(1) = E[N]$  hence
    \[
    E[S_N] = E[N] \cdot E[X] = E[N]E[X].
    \]

    \item The variance $Var(S_N)$ is obtained by the second derivative of $\psi_{S_N}(t)$:
    \[
    Var(S_N) = \left.\frac{d^2}{dt^2}\psi_{S_N}(t)\right|_{t=0}.
    \]
    Applying the chain rule,
    \[
    \frac{d^2}{dt^2}\psi_{S_N}(t) = g_N''(\psi_X(t)) \cdot (\psi_X'(t))^2 + g_N'(\psi_X(t)) \cdot \psi_X''(t).
    \]
    Evaluating at $t = 0$ and using $\psi_X(0) = 1$, $\psi_X'(0) = E[X]$, and $\psi_X''(0) = E[X^2]$,
    \[
    Var(S_N) = g_N''(1) \cdot (E[X])^2 + g_N'(1) \cdot (E[X^2]).
    \]
    Since $g_N'(t) = E[Nt^{N-1}]$ and $g_N''(t) = E[N(N-1)t^{N-2}]$, when $t=0$ we get
    \[
    Var(S_N) = E[N(N-1)] \cdot (E[X]^2) + E[N] \cdot (E[X^2]) =E[X^2](E[N^2] -E[N]^2) +E[N](E[X^2]- E[X]^2) 
    \]
    Simplifying yields \(E[N]Var[X] + E[X]^2Var[N].\)
\end{enumerate}


\exercise
Prove that \(\varphi_{S_N}(t) = g_N(\varphi_X(t))\)

\solution
\[
    \varphi_{S_N}(t) = E[e^{itS_N}] = \sum E[e^{itS_N} | N = n] P(N=n) = \sum E[e^{itS_n} | N = n] P(N=n) = \sum E[e^{itS_n}] P(N=n)
\]
\[
    \sum E[e^{itS_n}]P(N=n)   = \sum E[e^{it(X_1+X_2 + ... X_n)}]P(N=n) = \sum E[\bigl(e^{it(X)}\bigr)^n]P(N=n) = \sum E[e^{it(X)}]^nP(N=n)
\]
\[
    \sum \varphi_X(t)^n P(N=n) = E[\varphi_X(t)^N]   = g_N(\varphi_X(t))
\]


\exercise
Use the result from 6.4 do do exercise 6.3 again.

\solution
Im so darn tired ok fine ill do it. No actually i wont, its literally just 6.3 but keep the $-$ signs from $i$ in mind

\subsection{Branching Process}

\exercise

\begin{enumerate}
    \item Prove that $E[X(n)] = (E[Y])^n$
    \item Prove that $VarX(n) = \sigma^2(m^{n-1}+m^{n} + ... + m^{2n-2})$
\end{enumerate}

\solution
\begin{enumerate}
    \item We know that \(g_{n}(t) = g_{n-1}(g_{Y_1}(t)) \), and since $X(0)=1$ we have $X(1) = Y$. We also know the base case, $g_2(t)=g_1(g_1(t))$. By induction we have
\[
  g_n(t) = g_{n-1}(g_1(t)) \rightarrow g_{n+1} = g_{n}(g_1(t)) = g_{n-1}(g_1(g_1(t))) = g_{n-1}(g_2(t))
\]
\item The law of total variance states 
\[
    Var X(n) = E[Var(X(n)|X(n-1))] + Var(E[X(n)|X(n-1)]) \]

Which can be rewritten as (using the fact that $Var(Y) = \sigma^2$ and $E[X(n)] = m^n$)
\[
Var X(n) = E[Var(Y_1 + Y_2 + ... Y_{X(n-1)})] + Var(E[Y_1 + Y_2 + ... Y_{X(n-1)}]) =\]
\[
     E[X(n-1)Var(Y)] + Var(X(n-1)E[Y]) = \sigma^2E[X(n-1)] + m^2Var(X(n-1))
      \]

    Together with $Var(X(0)) = 0$, we get our result by induction.

Another way to prove it is through generating functions.

% k

% We know that a trunkated geometric series can be written as
% \item \[
%     x^n + x^{n+1} + ... +x^{2n-2}
% \]
\end{enumerate}


\subsection{Problems}

\problem  %problem 1
The nonnegative, integer-valued random variable $X$ has generating function $g_X(t) = \log\left(\frac{1}{1 - qt}\right)$. Determine $P(X = k)$ for $k = 0, 1, 2$, $\mathbb{E}X$, and $\mathrm{Var}X$.


\solution
Given the generating function for a random variable $X$:
\[
g_X(t) = \ln\left(\frac{1}{1 - qt}\right) = -\ln(1-qt),
\]
we first normalize the generating function by setting $g_X(1) = 1$, leading to:
\[
q = 1 - e^{-1}.
\]
The generating function can be expanded as:
\[
-\log(1 - qt) = \sum_{k=1}^\infty \frac{(qt)^k}{k} = \sum_{k=1}^\infty t^k \frac{(1 - e^{-1})^k}{k}.
\]
This gives us the probability mass function:
\[
P(X = k) = \frac{(1 - e^{-1})^k}{k} \quad \text{for } k \geq 1, \text{ and } P(X = 0) = 0.
\]

Taking the derivative yields 
\[
  EX = g_X'(1) = -(1-(1-e^{-1})\cdot1)^{-1}(1-(1-e^{-1})\cdot1) = e-1
\]

To calculate the variance \(\mathrm{Var}(X)\):
\[
\mathrm{Var}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2,
\]
where \(\mathbb{E}(X^2)\) involves calculating the second derivative of the generating function:
\[
g_X''(t) = \frac{d}{dt} \left( \frac{q}{1 - qt} \right) = \frac{q^2}{(1-qt)^2},
\]
evaluated at \(t = 1\):
\[
g_X''(1) = \frac{(1-e^{-1})^2}{(1-(1-e^{-1}))^2} = e^2 - 2e + 1,
\]
so,
\[
\mathbb{E}(X^2) = \mathbb{E}(X(X-1)) + \mathbb{E}(X) = (e^2 - 2e + 1) + (e-1) = e^2 - e,
\]
and therefore,
\[
\mathrm{Var}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2 = (e^2 - e) - (e-1)^2 = e - 1.
\]

\problem % 2
The random variable $X$ has the property that all moments are equal, i.e., $\mathbb{E}X^n = c$ for all $n \geq 1$, for some constant $c$. Find the distribution of $X$ (no proof of uniqueness is required).

\solution Since uniqueness is not required, its suficcient to find one solution. We are given the differential equation.

\[
\psi_X(t) = \psi_X'(t).
\]
Which has the trivial solution \[
    \psi_X(t) = e^t = 0 + 1e^t \sim \psi_{Be(1)}(t)
\]


\

\problem % 3
The random variable $X$ has the property that $\mathbb{E}X^n = \frac{2^n}{n+1}$, for $n = 1, 2, \dots$ Find some (in fact, the unique) distribution of $X$ having these moments.

\problem % 4
Suppose that $Y$ is a random variable such that $\mathbb{E}Y^k = \frac{1}{4} + 2^{k-1}$, for $k = 1, 2, \dots$ Determine the distribution of $Y$.

\problem % 5
Let $Y \sim \beta(n,m)$, $(n, m)$ integers 
\begin{enumerate}
    \item Compute $ \psi_{-\log Y}(t)$
    \item Show that $-\log Y$ has the same distribution as $S = \sum_{k=1}^{m}X_k$ where $X_k \sim Exp()$
\end{enumerate}







\solution

\begin{enumerate}
    \item \[
        \psi_{-\log Y}(t) =  E[e^{-\log(Y)t}] = E[\exp\{log(Y^{-t})\}] = E[Y^{-t}]
    \]
    Using the fact thats
    \[f_Y(y)=\frac{y^{n-1}(1-y)^{m-1}}{ \mathrm{B}(n,m)}\hspace{1em}; \hspace{1em} \mathrm{B}(\alpha ,\beta) = \int_{0}^{1} y^{\alpha-1} (1-y)^{\beta-1} dy
    \] 
    Where $ \mathrm {B}$ is the Beta function, we get the integral
\[
    \psi_{-\log Y}(t) = \int_{0}^{1} y^{-t} f_Y(y) dy  = \int_{0}^{1} y^{-t} \frac{1}{\mathrm{B}(n,m)} y^{n-1} (1-y)^{m-1} dy
\]


\[
    = \frac{1}{\mathrm{B}(n,m)} \int_{0}^{1} y^{(n-t)-1} (1-y)^{m-1} dy = \frac{\mathrm{B}(n-t,m)}{\mathrm{B}(n,m)} = \frac{\frac{\Gamma(n-t)\Gamma(m)}{\Gamma(n-t+m)}}{\frac{\Gamma(n)\Gamma(m)}{\Gamma(n+m)}} =
     \frac{\Gamma(n+m)\Gamma(n-t)}{\Gamma(n+m-t)\Gamma(n)}
\]
Despite the problem description, this equation works regardless of $n,m,t$, since you can factor out the non-integer parts from the Gamma functions. Regardless; 
\[
    =\frac{\Gamma(n+m)}{\Gamma(n)} \frac{\Gamma(n-t)}{\Gamma(n+m-t)} = \frac{(n+m-1)(n+m-2)...(n+1)(n) }{(n+m-t-1)(n+m-t-2)...(n-t+1)(n-t)}
\]
\[
    = \frac{n+m-1}{n+m-1-t}\cdot\frac{n+m-2}{n+m-2-t} ...\cdot\frac{n}{n-t} = \prod_{k=0}^{m-1} \frac{n+k}{n+k-t}
\]
\item We want to prove \[
  \psi_S(t) = \psi_{X_1 + ... X_m}(t) = \psi_{X_1}(t)\psi_{X_2}(t)...\psi_{X_m}(t) = \psi_{Exp(\lambda_1)}(t)\psi_{Exp(\lambda_2)}(t)...\psi_{Exp(\lambda_m)}(t)
\]
We simply use the result form (a)
\[ 
    \frac{n+k}{n+k-t} = \frac{1/(n+k)}{1/(n+k)} \frac{n+k}{n+k-t} = \frac{1}{1-\frac{t}{n+k}} = \psi_{Exp(n+k)(t)}
\]
Which proves (b)
\end{enumerate}

\problem % 6
 Show, by using MGF's, that if $X \sim L(1)$ and $Y \sim Exp(1)$ then $X \overset{d}{=} Y_1 - Y_2$




To show that $X \sim L(1)$ is equidistributed with $Y_1 - Y_2$ where $Y_1, Y_2 \sim Exp(1)$, we use the characteristic functions (CF).
\solution
The CF for the Laplace distribution $L(a)$ is:
\[
\varphi_X(t) = \frac{1}{1 + a^2 t^2} = \varphi_X(t) = \frac{1}{1 + t^2}
\]
For $a=1$. The CF for the Exponential distribution $Exp(\lambda)$ is:
\[
\varphi_Y(t) = \frac{1}{1 - it} =
\varphi_Y(t) = \frac{1}{1 - it/\lambda}
\]
For $\lambda=1$. Since $Y_1$ and $Y_2$ are independent:
\[
\varphi_{Y_1 - Y_2}(t) = \varphi_{Y_1}(t) \cdot \varphi_{-Y_2}(t) = \left(\frac{1}{1 - it}\right) \left(\frac{1}{1 + it}\right) = \frac{1}{1 + t^2}
\]

Since:
\[
\varphi_X(t) = \varphi_{Y_1 - Y_2}(t) = \frac{1}{1 + t^2}
\]
We conclude that $X \overset{d}{=} Y_1 - Y_2$, indicating that they are equidistributed.


\setcounter{problem}{21}
\problem 
Let \( N, X_1, X_2, \dots \) be independent random variables such that 
\( N \sim \text{Po}(1) \) and \( X_k \sim \text{Po}(2) \) for all \( k \). 
Define \( Z = \sum_{k=1}^N X_k \) (and \( Z = 0 \) when \( N = 0 \)). Compute
\begin{enumerate}
    \item \( E(Z) \)
    \item\( \text{Var}(Z) \) 
    \item \( P(Z = 0) \)
\end{enumerate}

\solution
We know that \[
    g_Z(t) = g_N(g_X(t)) = g_N(e^{2(t-1)}) = \exp\{e^{2(t-1)}-1\}\]

\begin{enumerate}
    \item \[EZ =  g_Z'(1) = \exp\{e^{2(1-1)}-1\} \cdot e^{2(1-1)} \cdot 2 = e^0 e^0 2 = 2
     \] Or we can use theorem 6.2
     \[
        E[S_N] = E[N] E[X] = 2\cdot1 = 2
     \]
     \item \[
        Var(S_N) = E[N]Var(X)+E[X]^2Var(N) = 1\cdot2 + 4\cdot1 = 6
     \]
     \item \[ P(Z=0) = g_Z(0) =   \exp\{e^{2(0-1)}-1\}  = exp\{e^{-2}-1\} \]
\end{enumerate}
% Set the counter to 42 before the first problem
\setcounter{problem}{25}

\problem
Same as before, passangers in cars. $B\sim Po(b)$, $P\sim Po(p)$. $Y\sim$ number of passangers in an hour. Calculate:
\begin{enumerate}
    \item \( g_Y(t) \)
    \item\( \text{E}(Z) \) 
    \item\( \text{Var}(Z) \) 
\end{enumerate}
\solution
\begin{enumerate}
    \item \[
      g_Y(t) = g_B(g_P(t)) = g_B(e^{p(t-1)}) = \exp\{b(e^{p(t-1)} - 1)\}  
    \]
    \item
     \[
        E[Y] = E[B] E[P] = bp
     \]
     \item \[
        Var(Y) = E[B]Var(P)+E[P]^2Var(B) = bp + bp^2 = bp(1+p)
     \]
\end{enumerate}
Also, interesting that the number of passangers is $Po(p)$, it suggests the possibility of having 0 passangers in a car. Driverless cars, the future is now i guess hehe.
\problem
\problem
 Lisa shoots at a target. The probability of a hit in each shot is $\frac{1}{2}$. 
 Given a hit, the probability of a bull's-eye is $p$. She shoots until she misses the target.
  Let $S_N$ be the total number of bull's-eyes Lisa has obtained when she has finished shooting,
  find its distribution.

\solution
Yet another problem yet exactly the same. Since she wont have a bulls-eye when she misses, $N \sim G(1/2)$

\[
  g_{S_N} = g_N(g_X(t)) = g_N(q+pt) = \frac{.5}{1-.5(q+pt)} = \frac{1}{2-(1-p + pt)} = \frac{1/(p+1)}{1-p/(p+1) t}  = \psi_{G(1/p+1)(t)}
\]



\problem

\problem 
Philip throws a fair die until he obtains a four.
 Diane then tosses a coin as many times as Philip
  threw his die. Determine the expected value and variance of the number of.
\begin{enumerate}
    \item heads
    \item tails
    \item heads and tails obtained by Diane
\end{enumerate}
\solution
Let $N \sim F(1/6)$ be $\#$Philips throws, $X_k \sim Be(0.5)$ be the $k$:th coinflip and $S_N = X_1 + ... X_N$.
\begin{enumerate}
    \item \[
        E[S_N] = E[N]E[X] = 6\cdot.5 = 3
    \]
    \[
        Var(S_N) = E[N]Var(X) + E[X^2]Var(N) = 6\cdot0.25 + 0.25\cdot\frac{5/6}{(1/6)^2} =9
        \]
    \item same as (a) by symetry
    \item since were counting both heads and tails, its the same as just counting all coinflips, ie how many dice rolls Philip makes
    \[
      E[N] = 2\cdot3 = 6\text{(linearity of expectation)} \]
       \[
      Var(N) = \frac{5/6}{(1/6)^2}  = 30
    \]
\end{enumerate}
\setcounter{problem}{34}

\problem
Suppose that the offspring distribution in a branching process is $Ge(p)$-distributed, and let $X(n)$ be the number of individuals in generation $n, n=0,1,2...$.

\begin{enumerate}
    \item Whats the probability of extinction
    \item Find the prob that $X(2) = 0$
\end{enumerate}

\solution

\begin{enumerate}
    \item The solutions will be $t=g_{Ge(p)}(t)$, ie \[
    t = \frac{p}{1-qt} = \frac{1-q}{1-qt} \rightarrow 1-q = t(1-qt) \rightarrow t_1 = 1, t_2 = \frac{p}{q}    
    \] So, if $E[X] \leq 1$ it will be $1$, otherwise it will be $p/q$
    \item Probability of being extinct by $n=2$ is $P(X(2) = 0)$. \[
        P(X(2) = 0) = g_{X(2)}(0) = g_X(g_X(0)) = g_X\Bigl(\frac{p}{1-q \cdot 0}\Bigr) = \frac{p}{1-qp}
    \]
\end{enumerate}
\problem

\problem
Consider a branching process $\{X_n\}$ with offspring probabilities given by the table below:
\begin{center}
\begin{tabular}{c|c|c|c}
    $k$ & 0 & 1 & 2 \\ 
\hline
$p_k$ & $\frac{1}{6} $&$ \frac{1}{3}$ & $\frac{1}{2}$ \\ 
    
\end{tabular}
\end{center}
\begin{enumerate}
\item Determine the prob of extinction
\item Probability of the population being extinct in the $2$nd generation
\item The expected number of children given there are no grandchildren

\end{enumerate}


\solution
\begin{enumerate}
    \item \[
    t = \frac{1}{6}t^0 + \frac{1}{3}t + \frac{1}{2}t^2 \rightarrow t_1=1, t_2=\frac{1}{3}
    \]    
    \item \[
    g_X(t) = E[t^X] = \frac{t^0}{6} + \frac{t}{3} + \frac{t^2}{2} 
    \]
    \[
        g_{X(2)}(0) = g_X(g_X(0)) = g_X\Bigl(\frac{1}{6}\Bigr) = \frac{1}{6} + \frac{1}{18} + \frac{1}{72} = \frac{12+4+1}{72} = \frac{17}{72}     
    \]
    \item i dont know if theres a way to prove this by MGF's, but using kolgomorov:
    \[
        P(X(1)=x|X(2)=0) = \frac{P(X(2)=0|X(1)=x) P(X(1)=x)}{P(X(2)=0)} = \frac{\Bigl(\frac{1}{6}\Bigr)^x p_x}{\frac{17}{72}}
    \]
    \[
      E[X(1)|X(2)=0] = \sum x P(X(1)=x|X(2)=0)  = 
      \sum x \frac{\Bigl(\frac{1}{6}\Bigr)^x p_x}{\frac{17}{72}} =
      0\frac{(1/6)^0\frac{1}{6}}{17/72} + 1\frac{(1/6)^1\frac{1}{3}}{17/72} + 2\frac{(1/6)^2\frac{1}{2}}{17/72}
    \]
    \[
      = 0 + \frac{72}{18 \cdot 17} + \frac{72}{36 \cdot 17} = \frac{6}{17} 
    \]

\end{enumerate}


\problem
\problem
\problem
The growth dynamics of pollen cells can be modeled by binary splitting as follows: After one unit of time, a cell either splits into two or dies. The new cells develop according to the same law independently of each other. The probabilities of dying and splitting are 0.46 and 0.54, respectively.

\begin{enumerate}
    \item Determine the maximal initial size of the population in order for the probability of extinction to be at least 0.3.
    \item What is the probability that the population is extinct after two generations if the initial population is the maximal number obtained in (a)?
\end{enumerate}

\solution
\begin{enumerate}
    \item One pollen cell has a $\frac{46}{54}$ probability of dying. Since each cell is independent, we want:
     \[
        \frac{46}{54}^k = \frac{1}{3} \rightarrow k = \log_{46/54}(\frac{1}{3}) \approx 7
    \]
    \item The PGF of a single cell $Y$ is $g_Y(t)=0.46+0.54t^2$. We want to find \[
        P(X(2) = 0)= g_{X(2)}(0) = g_{X(1)}(g_Y(0)) = g_{X(1)}(.46) = g_{7}(g_Y(.46)) = g_7(.46+.54(.46)^2)
        \]
        \[
            =
            (.46+.54(.46)^2)^7 \approx .0205    
        \]
\end{enumerate}

\setcounter{problem}{42}

\problem Consider a branching process with a Poisson-distributed offspring with mean \( m \). Let \( X(1) \) and \( X(2) \) be the number of individuals in generations 1 and 2, respectively. Determine the generating function of:
\begin{itemize}
    \item[(a)] \( X(1) \),
    \item[(b)] \( X(2) \),
    \item[(c)] \( X(1) + X(2) \),
    \item[(d)] Determine \( \text{Cov}(X(1), X(2)) \).
\end{itemize}

\solution
\begin{enumerate}
    \item The generating function \( G_{X(1)}(s) \) for \( X(1) \) is:
    \[ g_{X(1)}(s) = e^{m(s-1)} \]

    \item darn
    \item 
\end{enumerate}
% Part (a)

% Part (b)
The generating function \( G_{X(2)}(s) \) for \( X(2) \), where \( X(1) \) individuals each follow a Poisson distribution:
\[ G_{X(2)}(s) = e^{m(e^{m(s-1)}-1)} \]

% Part (c)
The generating function \( G_{X(1) + X(2)}(s) \) for \( X(1) + X(2) \) is:
\[ G_{X(1)+X(2)}(s) = e^{m(s-1) + m(e^{m(s-1)}-1)} \]

% Part (d)
The covariance \( \text{Cov}(X(1), X(2)) \) between \( X(1) \) and \( X(2) \) is:
\[ \text{Cov}(X(1), X(2)) = E[X(1)X(2)] - E[X(1)]E[X(2)] = m^3 - m^2 \]
\end{enumerate}


\solution

The probability distribution for \( X \), given \( Y = p \) uniformly distributed over \([0,1]\), is computed by integrating:
\[ P(X = k) = \int_0^1 (1-p)^{k-1} p \, dp \]
Using the beta function, this can be evaluated as:
\[ P(X = k) = \frac{1}{(k)(k+1)} \]
Thus, \( X \) follows a distribution where \( P(X = k) = \frac{1}{k(k+1)} \) for \( k \geq 1 \).

