\section{Transforms}


\subsection{a}
\subsection{b}
\subsection{The Moment Generating Function}
\exercise
\exercise
\exercise
\exercise
\exercise
\begin{enumerate}[label=(\alph*)]
    \item Show that if \( X \sim N(\mu, \sigma^2) \), then \( \mathbb{E}[X] = \mu \) and \( \text{Var}(X) = \sigma^2 \).
    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. Show that \( X_1 + X_2 \) is normally distributed, and find the mean and variance of \( X_1 + X_2 \).
    \item Let \( X \sim N(0,\sigma^2) \). Show that for \( n = 0, 1, 2, \ldots \),
    \[
    \mathbb{E}[X^{2n+1}] = 0, 
    \]
    and 
    \[
    \mathbb{E}[X^{2n}] = (2n-1)!! \cdot \sigma^{2n} = 1 \cdot 3 \cdot 5 ... \cdot (2n-1) \cdot \sigma^{2n}.
    \]
    Here, \( (2n-1)!! \) denotes the double factorial of \( 2n-1 \).
\end{enumerate}
\solution
\begin{enumerate}[label=(\alph*)]
    \item Given a normal random variable \( X \sim N(\mu, \sigma^2) \), its characteristic function \( \psi_X(t) \) is expressed as:
    \[
    \psi_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
    \]
    The expected value \( \mathbb{E}[X] \) is the coefficient of \( t \) in the Taylor expansion of \( \psi_X(t) \) around \( t=0 \), which yields:
    \[
    \mathbb{E}[X] = \left. \frac{d}{dt}\psi_X(t) \right|_{t=0} = \mu.
    \]
    To find the variance \( \text{Var}(X) \), we compute the second derivative of \( \psi_X(t) \) at \( t=0 \):
    \[
    \text{Var}(X) = \left. \frac{d^2}{dt^2}\psi_X(t) \right|_{t=0} - (\mu)^2 = \sigma^2.
    \]

    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. To show that the sum \( X_1 + X_2 \) is also normally distributed and to find its parameters, consider their moment generating functions:

    \[
    \Psi_{X_1}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2}, \quad \Psi_{X_2}(t) = e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Since \( X_1 \) and \( X_2 \) are independent, the MGF of their sum is the product of their MGFs:
    
    \[
    \Psi_{X_1 + X_2}(t) = \Psi_{X_1}(t) \cdot \Psi_{X_2}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2} \cdot e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Simplify by combining the exponents:
    
    \[
    \Psi_{X_1 + X_2}(t) = e^{(\mu_1 + \mu_2) t + \frac{1}{2} (\sigma_1^2 + \sigma_2^2) t^2}.
    \]
    
    This is the MGF of a normal distribution with mean \( \mu_1 + \mu_2 \) and variance \( \sigma_1^2 + \sigma_2^2 \). Therefore, \( X_1 + X_2 \) follows a normal distribution \( N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \).
    \item 

    Let \( X \sim N(0, \sigma^2) \). The characteristic function \( \psi_X(t) \), which also serves as the moment generating function in this context, is given by:
    \[
    \psi_X(t) = e^{\frac{1}{2} \sigma^2 t^2}.
    \]
    Expanding \( \psi_X(t) \) using a Taylor series around \( t = 0 \) results in:
    \[
    \psi_X(t) = \sum_{n=0}^{\infty} \frac{\frac{1}{2}\sigma^2 t^2}{n!} t^{2n} = 1 + \frac{1}{2}\sigma^2 t^2 + \frac{(\frac{1}{2}\sigma^2 t^2)^2}{2!} + \frac{(\frac{1}{2}\sigma^2 t^2)^3}{3!} + \ldots
= 1 + \frac{\sigma^2 t^2}{2 } + \frac{\sigma^4 t^4}{2^2 \cdot 2!} + \frac{\sigma^6 t^6}{2^3 \cdot 3!} + \ldots
    \]
    
    This series only contains even powers of \( t \), confirming that all coefficients of odd powers of \( t \) are zero, thus:
    \[
    \mathbb{E}[X^{2n+1}] = 0
    \]
    for all odd powers \( 2n+1 \). This occurs because the derivatives of \( \psi_X(t) \) at \( t=0 \) for odd orders are zero, as each term in the expansion of \( \psi_X(t) \) contains even powers.
    
    For even powers, consider the coefficient of \( t^{2n} \) in the Taylor expansion:
    \[
    \mathbb{E}[X^{2n}] = \left. \frac{d^{2n}}{dt^{2n}} \psi_X(t) \right|_{t=0} = \left. \frac{d^{2n}}{dt^{2n}} \left( \sum_{k=0}^{\infty} \frac{1}{k!} \left(\frac{1}{2}\sigma^2 t^2\right)^k \right) \right|_{t=0}
    \]
    
    To see why \( \mathbb{E}[X^{2n}] \) equals \((2n-1)!! \sigma^{2n}\), take the \(2n\)-th derivative:
    \[
    \mathbb{E}[X^{2n}] = \frac{1}{n!} \left(\frac{1}{2} \sigma^2\right)^n \cdot 2^n \cdot (2n)! = \sigma^{2n} \cdot (2n-1)!!
    \]
    This computation correctly reflects the product of the double factorial \((2n-1)!!\) which is the product of all odd numbers up to \((2n-1)\), resulting in:
    \[
    (2n-1)!! = 1 \cdot 3 \cdot 5 \cdot \ldots \cdot (2n-1) \cdot (\sigma^{2n}).
    \]
    
\end{enumerate}


\exercise

\begin{itemize}
    \item[(a)] Show that if \( X \sim N(0, 1) \) then \( X^2 \sim \chi^2(1) \) by computing the moment generating function (MGF) of \( X^2 \), that is, by showing that
    \[
    \psi_{X^2}(t) = \mathbb{E}[\exp(tX^2)] = \frac{1}{\sqrt{1-2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]

    \item[(b)] Show that if \( X_1 \sim N(0, 1) \) and \( X_2 \sim N(0, 1) \) are independent, then \( X_1^2 + X_2^2 \) is distributed as \( \chi^2(2) \) (which is equivalent to an exponential distribution with mean 2).
\end{itemize}

\solution
\begin{enumerate}[label=(\alph*)]
    \item
    
    Begin by recognizing the integral for the MGF:
    \[
    \psi_{X^2}(t) = \int_{-\infty}^{\infty} e^{tx^2} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{x^2(t - \frac{1}{2})} \, dx.
    \]
    This integral converges for \(t < \frac{1}{2}\). Transform \( x \) to eliminate the variable change explicitly:
    \[
    \frac{d(x\sqrt{1 - 2t})}{dx} = \sqrt{1 - 2t}, \quad dx = \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}}
    \]
    Substitute directly:
    \[
    \psi_{X^2}(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x\sqrt{1 - 2t})^2}{2}} \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}} = \frac{1}{\sqrt{1 - 2t}}.
    \]
    The integral of the standard normal density over the transformed variable is 1, leading to the final MGF expression for \( X^2 \).
    \item[(b)] Given that \(X_1 \sim N(0, 1)\) and \(X_2 \sim N(0, 1)\) are independent, to show that \(X_1^2 + X_2^2\) is distributed as \(\chi^2(2)\), consider the moment generating functions (MGFs) of \(X_1^2\) and \(X_2^2\), which are:
    \[
    \psi_{X_1^2}(t) = \psi_{X_2^2}(t) = \frac{1}{\sqrt{1 - 2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]
    Since \(X_1^2\) and \(X_2^2\) are independent, the MGF of their sum, \(X_1^2 + X_2^2\), is the product of their MGFs:
    \[
    \psi_{X_1^2 + X_2^2}(t) = \psi_{X_1^2}(t) \cdot \psi_{X_2^2}(t) = \left(\frac{1}{\sqrt{1 - 2t}}\right)^2 = \frac{1}{1 - 2t}.
    \]
    This MGF, \(\frac{1}{1 - 2t}\), is the MGF of a \(\chi^2\) distribution with 2 degrees of freedom. The \(\chi^2(2)\) distribution is also known to be equivalent to an exponential distribution with mean 2, confirming the distribution of \(X_1^2 + X_2^2\).
\
\end{enumerate}


\subsection{The Characteristic Function}

\exercise

\begin{enumerate}[label=(\alph*)]
    \item
    For a Bernoulli random variable \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = q + p e^{it}, \quad \text{where } q = 1-p.
    \]
    
    \item 
    
    For a Binomial random variable \(Y \sim \text{Bin}(n,p)\):
    \[
    \varphi_{\text{Bin}(n,p)}(t) = (q + p e^{it})^n.
    \]
    
    \item 
    
    For a compound Poisson random variable \(Z\) with rate \(\lambda\) and jump size distribution \(C\):
    \[
    \varphi_{C}(t) = \frac{p}{1 - q e^{ist}},
    \]
    assuming a specific relationship between the parameters \(p\) and \(q\), and \(s\).
    
    \item 
    
    For a compound Poisson random variable \(W\) with intensity \(m\) and jump size distribution \(P\):
    \[
    \varphi_{P \ast \theta(m)}(t) = \exp\left[m(e^{it} - 1)\right].
    \]
\end{enumerate}
\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
    \item 
    \textbf{Bernoulli Distribution} \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = \mathbb{E}[e^{itX}] = \sum_{x=0}^1 e^{itx} \Pr(X = x) = e^{it \cdot 0} \Pr(X=0) + e^{it \cdot 1} \Pr(X=1) = (1-p) + pe^{it}.
    \]
    This is exactly the expression given: \(q + p e^{it}\), where \(q = 1-p\).
    
    \item 
    \textbf{Binomial Distribution} \(Y \sim \text{Bin}(n,p)\):
    The characteristic function of a sum of independent identically distributed random variables (by the property often called the \emph{factorization property}) is:
    \[
    \varphi_{\text{Bin}(n,p)}(t) = [\varphi_{\text{Be}(p)}(t)]^n = (q + pe^{it})^n.
    \]
    This uses the property that the characteristic function of the sum of independent random variables is the product of their characteristic functions.
    
    \item 
    \textbf{Geometric Distribution}:
    \[
    \varphi_{X}(t) = \mathbb{E}[e^{itX}] = \sum_{x=0}^\infty e^{itx} \Pr(X = x) = \sum_{x=0}^\infty e^{itx} \frac{p q^x}{1-q} = \frac{p}{1-qe^{it}},
    \]
    where we used the formula for the sum of a geometric series \(\sum_{x=0}^\infty ar^x = \frac{a}{1-r}\) applied to \(e^{it}\) as \(r\).
    

\item 
\textbf{Compound Poisson Distribution} (\( W \)) with intensity \( m \) and jump size distribution \( P \):

The compound Poisson variable \( W \) can be expressed as \( W = \sum_{k=1}^N X_k \), where \( N \sim \text{Poisson}(m) \) and \( X_k \) are iid random variables from the distribution \( P \). The characteristic function \( \varphi_{W}(t) \) is given by the expectation:
\[
\varphi_{W}(t) = \mathbb{E}[e^{itW}].
\]

Given \( W \) conditioned on \( N \) being equal to \( n \), the sum \( W = X_1 + X_2 + \cdots + X_n \) and the \( X_k \)'s are independent. So, we write:
\[
\mathbb{E}[e^{itW} \mid N=n] = \mathbb{E}[e^{it(X_1 + X_2 + \cdots + X_n)}] = \prod_{k=1}^n \mathbb{E}[e^{itX_k}] = (\varphi_P(t))^n,
\]
where \( \varphi_P(t) \) is the characteristic function of the distribution \( P \).

The unconditional expectation is:
\[
\varphi_{W}(t) = \sum_{n=0}^\infty \mathbb{E}[e^{itW} \mid N=n] \Pr(N = n) = \sum_{n=0}^\infty (\varphi_P(t))^n \frac{e^{-m} m^n}{n!}.
\]
Using the Taylor series expansion for the exponential function, we have:
\[
\varphi_{W}(t) = e^{-m} \sum_{n=0}^\infty \frac{[m \varphi_P(t)]^n}{n!} = e^{-m} e^{m \varphi_P(t)} = \exp[m(\varphi_P(t) - 1)].
\]

This directly ties into the idea you suggested, where each \( e^{itx} \) term is weighted by its Poisson probability, which then sums to form the exponential series representation of \( \varphi_{W}(t) \).




\exercise
\exercise
\begin{enumerate}[label=(\alph*)]
    \item Calculate the mean and variance of the Binomial distribution using its characteristic function.
    \item Calculate the mean and variance of the Poisson distribution using its characteristic function.
    \item Calculate the mean and variance of the Uniform distribution using its characteristic function.
    \item Calculate the mean and variance of the Exponential distribution using its characteristic function.
\end{enumerate}

\solution
\begin{enumerate}[label=(\alph*)]
    \item
    \textbf{Binomial Distribution}:
    \[
    \text{Characteristic Function:} \quad \varphi_X(t) = (1-p + pe^{it})^n
    \]
    \text{Expansion of} \( e^{it} \):
    \[
    e^{it} \approx 1 + it - \frac{t^2}{2}
    \]
    \text{Substitute and apply multinomial theorem:}
    \[
    \varphi_X(t) = (1-p + p(1 + it - \frac{pt^2}{2}))^n
    \]
    \text{Expand using multinomial coefficients:}
    \[
    \varphi_X(t) \approx \sum_{x,y,z}^n \binom{n}{x,y,z} (1-p)^x \left(pit\right)^y \left(-\frac{pt^2}{2}\right)^{z}
    \]
    \text{Relevant terms up to} \( t^2 \):
    \[
    \varphi_X(t) \approx \binom{n}{n,0,0}(1-p)^n + \binom{n}{n-1,1,0}(1-p)^{n-1}(pit) + \binom{n}{n-2,0,2}(1-p)^{n-2}\left(-\frac{pt^2}{2}\right)
    \]
    \text{Mean} \( E[X] \):
    \[
    E[X] = np
    \]
    \text{Variance} \( \operatorname{Var}(X) \):
    \[
    \operatorname{Var}(X) = np(1-p)
    \]

    \item
\textbf{Poisson Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = e^{\lambda (e^{it} - 1)}
\]
\text{Expansion of} \( e^{it} \):
\[
e^{it} \approx 1 + it - \frac{t^2}{2}
\]
\text{Substitute and expand:}
\[
\varphi_X(t) = e^{\lambda \left((1 + it - \frac{t^2}{2}) - 1\right)} = e^{\lambda (it - \frac{t^2}{2})}
\]
\text{Applying Taylor expansion to} \( e^{\lambda (it - \frac{t^2}{2})} \):
\[
\varphi_X(t) \approx 1 + \lambda (it - \frac{t^2}{2}) + \frac{\lambda^2}{2} (it - \frac{t^2}{2})^2 + \ldots
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + (\lambda^2 + \lambda) it - \frac{\lambda t^2}{2}
\]
\text{Mean} \( E[X] \):
\[
E[X] = \lambda
\]
\text{Second moment} \( E[X^2] \):
\[
E[X^2] = \lambda^2 - \lambda
\]
\text{Variance} \( \operatorname{Var}(X) \):
\[
\operatorname{Var}(X) = \lambda
\]


\item
\textbf{Uniform Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = \frac{e^{itb} - e^{ita}}{it(b-a)}
\]
\text{Expansions of} \( e^{itb} \) \text{and} \( e^{ita} \) (   \text{remember, we will divide by} \(it(b-a) \) \text{so we need terms up to} \(t^3\) ) : 
\[
e^{itb} \approx 1 + itb - \frac{t^2b^2}{2} - i\frac{t^3b^3}{6}, \quad e^{ita} \approx 1 + ita - \frac{t^2a^2}{2} - i\frac{t^3a^3}{6}
\]
\text{Substitute and simplify:}
\[
\varphi_X(t) = \frac{(1 + itb - \frac{t^2b^2}{2}- i\frac{t^3b^3}{6}) - (1 + ita - \frac{t^2a^2}{2}) - i\frac{t^3a^3}{6}}{it(b-a)}
\]
\[
\varphi_X(t) \approx \frac{1}{it(b-a)} \Bigl[ (1-1)+ it(b-a) -\dfrac{t^2}{2}(b^2-a^2) -i\dfrac{t^3}{6}(b^3-a^3) \Bigr] = \Bigl[ 0 + 1 +it\dfrac{b+a}{2}  + \dfrac{t^2}{6} (b^2+a^2 -ab) \Bigr]
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + it\frac{b+a}{2} - \frac{t^2(b^2 + a^2-ab)^2}{6}
\]
\text{Mean} \( E[X] \):
\[
E[X] = \frac{b+a}{2}
\]
\text{Second moment} \( E[X^2] \):
\[
E[X] = \frac{b^2+a^2-ab}{3}
\]
\text{Variance} \( \operatorname{Var}(X) \):
\[
\operatorname{Var}(X) = E[X^2] - E[X]^2 =  \frac{4(b^2+a^2-ab)}{4\cdot3} - \frac{3\cdot(b+a)^2}{3\cdot2^2}  = \frac{(b-a)^2}{12}
\]


\item
\textbf{Exponential Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = \frac{1}{1 - it/\lambda}
\]
\text{Expand using Gemoretric series}
\[
\frac{1}{1-x} = 1 +x + x^2 +x^3... \therefore \varphi_X(t) \approx 1 + it/\lambda + (it/\lambda)^2 + o(t^2)
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 + it \dfrac{1}{\lambda} - \frac{t^2}{2} \frac{2}{\lambda^2}
\]
\[
E[X] = \frac{1}{\lambda} \hspace{15pt} E[X^2] = \frac{2}{\lambda^2} \hspace*{15pt} \operatorname{Var}(X) = \frac{1}{\lambda^2}
\]


\item
\textbf{Standard Normal Distribution}:
\[
\text{Characteristic Function:} \quad \varphi_X(t) = e^{-\frac{t^2}{2}}
\]
\text{Apply Taylor expansion to} \( e^{-\frac{t^2}{2}} \):
\[
\varphi_X(t) \approx 1 - \frac{t^2}{2} + \frac{t^4}{8} - \frac{t^6}{48} + \ldots
\]
\text{Relevant terms up to} \( t^2 \):
\[
\varphi_X(t) \approx 1 - \frac{t^2}{2}
\]
\text{Which yields}:
\[
E[X] = 0 \hspace*{15pt} \operatorname{Var}(X) = 1
\]

\end{enumerate}


\end{enumerate}
