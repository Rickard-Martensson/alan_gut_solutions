\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb} % Essential for math environments and symbols
\usepackage{enumitem}
\title{Solutions for [Book Name]}
\author{Your Name}
\date{\today}
\usepackage{geometry}

% Setting smaller margins
\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}
\begin{document}

\maketitle

% Custom numbering setup
\renewcommand{\thesection}{\arabic{section}}
\newcounter{exercise}[section]
\newcounter{problem}[section]

\newcommand{\exercise}{
  \stepcounter{exercise}
  \subsection*{Exercise \thesection.\theexercise}
}

\newcommand{\problem}{
  \stepcounter{problem}
  \subsection*{Problem \thesection.\theproblem}
}

% Begin document content
% Chapter 1
\section{Chapter 1}

\exercise
Exercise description.
\subsubsection*{Solution}
Write your solution here.

\exercise
Exercise description.
\subsubsection*{Solution}
Another solution here.

% Chapter 2
\section{Chapter 2}

\exercise
Exercise description.
\subsubsection*{Solution}
Write your solution here.

\exercise
Exercise description.
\subsubsection*{Solution}
Another solution here.

% Chapter 3
\section{Chapter 3}

\exercise
\subsubsection*{Solution}

\exercise
\subsubsection*{Solution}
\exercise
\subsubsection*{Solution}
\exercise
\subsubsection*{Solution}
\exercise
\begin{enumerate}[label=(\alph*)]
    \item Show that if \( X \sim N(\mu, \sigma^2) \), then \( \mathbb{E}[X] = \mu \) and \( \text{Var}(X) = \sigma^2 \).
    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. Show that \( X_1 + X_2 \) is normally distributed, and find the mean and variance of \( X_1 + X_2 \).
    \item Let \( X \sim N(0,\sigma^2) \). Show that for \( n = 0, 1, 2, \ldots \),
    \[
    \mathbb{E}[X^{2n+1}] = 0, 
    \]
    and 
    \[
    \mathbb{E}[X^{2n}] = (2n-1)!! \cdot \sigma^{2n} = 1 \cdot 3 \cdot 5 ... \cdot (2n-1) \cdot \sigma^{2n}.
    \]
    Here, \( (2n-1)!! \) denotes the double factorial of \( 2n-1 \).
\end{enumerate}

\subsubsection*{Solution}
\begin{enumerate}[label=(\alph*)]
    \item Given a normal random variable \( X \sim N(\mu, \sigma^2) \), its characteristic function \( \psi_X(t) \) is expressed as:
    \[
    \psi_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
    \]
    The expected value \( \mathbb{E}[X] \) is the coefficient of \( t \) in the Taylor expansion of \( \psi_X(t) \) around \( t=0 \), which yields:
    \[
    \mathbb{E}[X] = \left. \frac{d}{dt}\psi_X(t) \right|_{t=0} = \mu.
    \]
    To find the variance \( \text{Var}(X) \), we compute the second derivative of \( \psi_X(t) \) at \( t=0 \):
    \[
    \text{Var}(X) = \left. \frac{d^2}{dt^2}\psi_X(t) \right|_{t=0} - (\mu)^2 = \sigma^2.
    \]

    \item Let \( X_1 \sim N(\mu_1, \sigma_1^2) \) and \( X_2 \sim N(\mu_2, \sigma_2^2) \) be independent random variables. To show that the sum \( X_1 + X_2 \) is also normally distributed and to find its parameters, consider their moment generating functions:

    \[
    \Psi_{X_1}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2}, \quad \Psi_{X_2}(t) = e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Since \( X_1 \) and \( X_2 \) are independent, the MGF of their sum is the product of their MGFs:
    
    \[
    \Psi_{X_1 + X_2}(t) = \Psi_{X_1}(t) \cdot \Psi_{X_2}(t) = e^{\mu_1 t + \frac{1}{2} \sigma_1^2 t^2} \cdot e^{\mu_2 t + \frac{1}{2} \sigma_2^2 t^2}.
    \]
    
    Simplify by combining the exponents:
    
    \[
    \Psi_{X_1 + X_2}(t) = e^{(\mu_1 + \mu_2) t + \frac{1}{2} (\sigma_1^2 + \sigma_2^2) t^2}.
    \]
    
    This is the MGF of a normal distribution with mean \( \mu_1 + \mu_2 \) and variance \( \sigma_1^2 + \sigma_2^2 \). Therefore, \( X_1 + X_2 \) follows a normal distribution \( N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \).
    \item 

    Let \( X \sim N(0, \sigma^2) \). The characteristic function \( \psi_X(t) \), which also serves as the moment generating function in this context, is given by:
    \[
    \psi_X(t) = e^{\frac{1}{2} \sigma^2 t^2}.
    \]
    Expanding \( \psi_X(t) \) using a Taylor series around \( t = 0 \) results in:
    \[
    \psi_X(t) = \sum_{n=0}^{\infty} \frac{\frac{1}{2}\sigma^2 t^2}{n!} t^{2n} = 1 + \frac{1}{2}\sigma^2 t^2 + \frac{(\frac{1}{2}\sigma^2 t^2)^2}{2!} + \frac{(\frac{1}{2}\sigma^2 t^2)^3}{3!} + \ldots
= 1 + \frac{\sigma^2 t^2}{2 } + \frac{\sigma^4 t^4}{2^2 \cdot 2!} + \frac{\sigma^6 t^6}{2^3 \cdot 3!} + \ldots
    \]
    
    This series only contains even powers of \( t \), confirming that all coefficients of odd powers of \( t \) are zero, thus:
    \[
    \mathbb{E}[X^{2n+1}] = 0
    \]
    for all odd powers \( 2n+1 \). This occurs because the derivatives of \( \psi_X(t) \) at \( t=0 \) for odd orders are zero, as each term in the expansion of \( \psi_X(t) \) contains even powers.
    
    For even powers, consider the coefficient of \( t^{2n} \) in the Taylor expansion:
    \[
    \mathbb{E}[X^{2n}] = \left. \frac{d^{2n}}{dt^{2n}} \psi_X(t) \right|_{t=0} = \left. \frac{d^{2n}}{dt^{2n}} \left( \sum_{k=0}^{\infty} \frac{1}{k!} \left(\frac{1}{2}\sigma^2 t^2\right)^k \right) \right|_{t=0}
    \]
    
    To see why \( \mathbb{E}[X^{2n}] \) equals \((2n-1)!! \sigma^{2n}\), take the \(2n\)-th derivative:
    \[
    \mathbb{E}[X^{2n}] = \frac{1}{n!} \left(\frac{1}{2} \sigma^2\right)^n \cdot 2^n \cdot (2n)! = \sigma^{2n} \cdot (2n-1)!!
    \]
    This computation correctly reflects the product of the double factorial \((2n-1)!!\) which is the product of all odd numbers up to \((2n-1)\), resulting in:
    \[
    (2n-1)!! = 1 \cdot 3 \cdot 5 \cdot \ldots \cdot (2n-1) \cdot (\sigma^{2n}).
    \]
    
\end{enumerate}

\exercise

\begin{itemize}
    \item[(a)] Show that if \( X \sim N(0, 1) \) then \( X^2 \sim \chi^2(1) \) by computing the moment generating function (MGF) of \( X^2 \), that is, by showing that
    \[
    \psi_{X^2}(t) = \mathbb{E}[\exp(tX^2)] = \frac{1}{\sqrt{1-2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]

    \item[(b)] Show that if \( X_1 \sim N(0, 1) \) and \( X_2 \sim N(0, 1) \) are independent, then \( X_1^2 + X_2^2 \) is distributed as \( \chi^2(2) \) (which is equivalent to an exponential distribution with mean 2).
\end{itemize}


\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
    \item
    
    Begin by recognizing the integral for the MGF:
    \[
    \psi_{X^2}(t) = \int_{-\infty}^{\infty} e^{tx^2} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{x^2(t - \frac{1}{2})} \, dx.
    \]
    This integral converges for \(t < \frac{1}{2}\). Transform \( x \) to eliminate the variable change explicitly:
    \[
    \frac{d(x\sqrt{1 - 2t})}{dx} = \sqrt{1 - 2t}, \quad dx = \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}}
    \]
    Substitute directly:
    \[
    \psi_{X^2}(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x\sqrt{1 - 2t})^2}{2}} \frac{d(x\sqrt{1 - 2t})}{\sqrt{1 - 2t}} = \frac{1}{\sqrt{1 - 2t}}.
    \]
    The integral of the standard normal density over the transformed variable is 1, leading to the final MGF expression for \( X^2 \).
    \item[(b)] Given that \(X_1 \sim N(0, 1)\) and \(X_2 \sim N(0, 1)\) are independent, to show that \(X_1^2 + X_2^2\) is distributed as \(\chi^2(2)\), consider the moment generating functions (MGFs) of \(X_1^2\) and \(X_2^2\), which are:
    \[
    \psi_{X_1^2}(t) = \psi_{X_2^2}(t) = \frac{1}{\sqrt{1 - 2t}} \quad \text{for} \quad t < \frac{1}{2}.
    \]
    Since \(X_1^2\) and \(X_2^2\) are independent, the MGF of their sum, \(X_1^2 + X_2^2\), is the product of their MGFs:
    \[
    \psi_{X_1^2 + X_2^2}(t) = \psi_{X_1^2}(t) \cdot \psi_{X_2^2}(t) = \left(\frac{1}{\sqrt{1 - 2t}}\right)^2 = \frac{1}{1 - 2t}.
    \]
    This MGF, \(\frac{1}{1 - 2t}\), is the MGF of a \(\chi^2\) distribution with 2 degrees of freedom. The \(\chi^2(2)\) distribution is also known to be equivalent to an exponential distribution with mean 2, confirming the distribution of \(X_1^2 + X_2^2\).
\
\end{enumerate}



% Chapter 4
\section{Chapter 4}

\exercise

\begin{enumerate}[label=(\alph*)]
    \item
    For a Bernoulli random variable \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = q + p e^{it}, \quad \text{where } q = 1-p.
    \]
    
    \item 
    
    For a Binomial random variable \(Y \sim \text{Bin}(n,p)\):
    \[
    \varphi_{\text{Bin}(n,p)}(t) = (q + p e^{it})^n.
    \]
    
    \item 
    
    For a compound Poisson random variable \(Z\) with rate \(\lambda\) and jump size distribution \(C\):
    \[
    \varphi_{C}(t) = \frac{p}{1 - q e^{ist}},
    \]
    assuming a specific relationship between the parameters \(p\) and \(q\), and \(s\).
    
    \item 
    
    For a compound Poisson random variable \(W\) with intensity \(m\) and jump size distribution \(P\):
    \[
    \varphi_{P \ast \theta(m)}(t) = \exp\left[m(e^{it} - 1)\right].
    \]
\end{enumerate}
\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
    \item 
    \textbf{Bernoulli Distribution} \(X \sim \text{Be}(p)\):
    \[
    \varphi_{\text{Be}(p)}(t) = \mathbb{E}[e^{itX}] = \sum_{x=0}^1 e^{itx} \Pr(X = x) = e^{it \cdot 0} \Pr(X=0) + e^{it \cdot 1} \Pr(X=1) = (1-p) + pe^{it}.
    \]
    This is exactly the expression given: \(q + p e^{it}\), where \(q = 1-p\).
    
    \item 
    \textbf{Binomial Distribution} \(Y \sim \text{Bin}(n,p)\):
    The characteristic function of a sum of independent identically distributed random variables (by the property often called the \emph{factorization property}) is:
    \[
    \varphi_{\text{Bin}(n,p)}(t) = [\varphi_{\text{Be}(p)}(t)]^n = (q + pe^{it})^n.
    \]
    This uses the property that the characteristic function of the sum of independent random variables is the product of their characteristic functions.
    
    \item 
    \textbf{Compound Poisson Distribution} (Specific Case):
    \[
    \varphi_{C}(t) = \mathbb{E}[e^{itC}] = \sum_{x=0}^\infty e^{itx} \Pr(C = x) = \sum_{x=0}^\infty e^{itx} \frac{p q^x}{1-q} = \frac{p}{1-qe^{it}},
    \]
    where we used the formula for the sum of a geometric series \(\sum_{x=0}^\infty ar^x = \frac{a}{1-r}\) applied to \(e^{it}\) as \(r\).
    
    \item 
    \textbf{Poisson Distribution}:
    Assuming a Poisson process with parameter \(\lambda\) and counting the number of events \(N\) with intensity \(m\):
    \[
    \varphi_{P \ast \theta(m)}(t) = \mathbb{E}\left[e^{it \sum_{k=1}^N X_k}\right] = \mathbb{E}\left[\prod_{k=1}^N e^{itX_k}\right] = \exp\left[m(e^{it} - 1)\right].
    \]
    This follows from the definition of the characteristic function of the Poisson distribution and using the independence of increments \(X_k\). Each \(X_k\) contributes \(e^{it}\) to the product, and the expectation over the Poisson-distributed count \(N\) produces the exponential function.
    
\end{enumerate}

\exercise
Exercise description.
\subsubsection*{Solution}
Another solution here.

\end{document}
